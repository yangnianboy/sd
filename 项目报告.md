# 基于扩散模型的多视角视觉错觉图像生成系统

## 项目报告

---

## 一、项目概述

### 1.1 项目背景

视觉错觉（Optical Illusion）是一种利用人类视觉感知特性创造的艺术形式。传统的视觉错觉图像需要艺术家精心设计，耗时且需要专业技能。随着深度学习技术的发展，特别是扩散模型（Diffusion Models）在图像生成领域取得的突破性进展，利用人工智能自动生成视觉错觉图像成为可能。

本项目基于 Visual Anagrams 方法，实现了一个多视角光学错觉图像生成系统。该系统能够生成在不同观察角度下呈现完全不同语义内容的图像，例如：一张图片正向观看是"雪山村庄"，旋转90度后则呈现为"一匹马"。

### 1.2 项目目标

1. 实现基于 DeepFloyd IF 扩散模型的多视角错觉图像生成
2. 支持多种几何变换类型（旋转、翻转、颜色反转等）
3. 提供从64×64到1024×1024的多分辨率图像输出
4. 支持生成展示错觉效果的动画视频
5. 针对显存受限环境进行内存优化

---

## 二、技术原理

### 2.1 扩散模型数学基础

#### 2.1.1 前向扩散过程（Forward Diffusion Process）

扩散模型通过逐步添加高斯噪声将数据分布 $q(x_0)$ 转换为标准高斯分布 $q(x_T) \approx \mathcal{N}(0, \mathbf{I})$。

**离散时间步扩散**：

$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})$$

其中 $\beta_t \in (0,1)$ 是噪声调度参数，控制每一步添加的噪声量。

**累积扩散**（通过重参数化技巧）：

$$q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)\mathbf{I})$$

其中：
- $\alpha_t = 1 - \beta_t$
- $\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s$
- $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$，$\epsilon \sim \mathcal{N}(0, \mathbf{I})$

#### 2.1.2 逆向去噪过程（Reverse Denoising Process）

模型学习从噪声 $x_T$ 逐步恢复原始数据 $x_0$：

$$p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

**DDPM 去噪公式**：

$$x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t)\right) + \sigma_t z$$

其中：
- $\epsilon_\theta(x_t, t)$ 是神经网络预测的噪声
- $z \sim \mathcal{N}(0, \mathbf{I})$ 是随机噪声（采样时）
- $\sigma_t$ 是方差调度参数

#### 2.1.3 训练目标

扩散模型通过预测添加的噪声进行训练：

$$\mathcal{L} = \mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon - \epsilon_\theta(x_t, t)\|^2\right]$$

其中 $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$，$t \sim \text{Uniform}(1, T)$。

### 2.2 条件扩散与分类器引导

#### 2.2.1 条件生成

在文本到图像生成中，扩散过程被条件化为文本提示 $c$：

$$p_\theta(x_{t-1}|x_t, c) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t, c), \Sigma_\theta(x_t, t, c))$$

#### 2.2.2 分类器引导（Classifier Guidance）

通过梯度引导增强条件控制：

$$\nabla_{x_t} \log p(x_t|c) = \nabla_{x_t} \log p(x_t) + s \cdot \nabla_{x_t} \log p(c|x_t)$$

其中 $s$ 是引导强度（guidance scale），本项目设置为 10.0。

**引导去噪公式**：

$$\epsilon_\theta(x_t, t, c) = \epsilon_\theta(x_t, t) - s \cdot \sqrt{1-\bar{\alpha}_t} \cdot \nabla_{x_t} \log p(c|x_t)$$

#### 2.2.3 无分类器引导（Classifier-Free Guidance）

更常用的方法，直接训练条件模型：

$$\tilde{\epsilon}_\theta(x_t, t, c) = \epsilon_\theta(x_t, t, \emptyset) + s \cdot (\epsilon_\theta(x_t, t, c) - \epsilon_\theta(x_t, t, \emptyset))$$

其中 $\emptyset$ 表示无条件（空提示）。

### 2.3 DeepFloyd IF 模型架构详解

#### 2.3.1 模型选择：像素空间 vs 潜在空间

**DeepFloyd IF（像素空间扩散）**：
- 直接在像素空间 $x \in \mathbb{R}^{H \times W \times 3}$ 操作
- 保持完整的空间信息，适合需要精确几何对应关系的任务
- 计算成本高，但质量更好

**Stable Diffusion（潜在空间扩散）**：
- 在压缩潜在空间 $z \in \mathbb{R}^{H/8 \times W/8 \times C}$ 操作
- 通过 VAE 编码器 $E: x \to z$ 和解码器 $D: z \to x$ 转换
- 计算效率高，但 VAE 编解码会引入伪影

**为什么选择像素空间**：
视觉错觉需要精确的像素级对应关系。VAE 的压缩-解压过程会破坏这种对应，导致变换后的图像质量下降。

#### 2.3.2 级联超分辨率架构

本项目采用三阶段级联架构：

| 阶段 | 模型 | 输入分辨率 | 输出分辨率 | 参数量 | 功能 |
|------|------|------------|------------|--------|------|
| Stage I | IF-I-L-v1.0 | 文本提示 | 64×64 | ~1.3B | 基础生成 |
| Stage II | IF-II-L-v1.0 | 64×64 | 256×256 | ~1.2B | 4×超分辨率 |
| Stage III | SD-x4-Upscaler | 256×256 | 1024×1024 | ~1.4B | 4×超分辨率 |

**级联优势**：
1. **计算效率**：低分辨率生成速度快，高分辨率仅用于细节增强
2. **质量保证**：每阶段专注于特定分辨率的特征学习
3. **内存优化**：可以分阶段加载模型，避免同时驻留所有参数

#### 2.3.3 T5 文本编码器

DeepFloyd IF 使用 T5-XXL（11B 参数）作为文本编码器：

$$c = \text{T5-Encoder}(\text{tokenize}(prompt))$$

- 输入：文本提示（如 "painting of a Snow Mountain"）
- 输出：文本嵌入 $c \in \mathbb{R}^{77 \times 4096}$（77 个 token，每个 4096 维）
- 优势：强大的语言理解能力，支持复杂语义描述

### 2.4 多视角去噪算法核心原理

#### 2.4.1 问题形式化

给定：
- 文本提示集合：$\mathcal{P} = \{p_1, p_2, ..., p_n\}$
- 视角变换集合：$\mathcal{V} = \{v_1, v_2, ..., v_n\}$，其中 $v_i: \mathbb{R}^{H \times W \times 3} \to \mathbb{R}^{H \times W \times 3}$

目标：生成图像 $x_0$，使得：
- $v_1(x_0)$ 在提示 $p_1$ 下看起来合理
- $v_2(x_0)$ 在提示 $p_2$ 下看起来合理
- ...
- $v_n(x_0)$ 在提示 $p_n$ 下看起来合理

#### 2.4.2 多视角噪声预测融合

**核心思想**：在每个去噪步骤 $t$，对图像应用不同的视角变换，分别预测噪声，然后融合。

**算法详细步骤**：

```
算法：多视角扩散采样（Multi-View Diffusion Sampling）
输入：文本提示 {p_1, p_2}, 视角变换 {v_1, v_2}
输出：满足多视角约束的图像 x_0

1. 初始化：x_T ~ N(0, I)
2. for t = T down to 1 do:
3.     // 对每个视角进行噪声预测
4.     for i = 1 to n do:
5.         x_t^(i) = v_i(x_t)                    // 应用视角变换
6.         ε_i = ε_θ(x_t^(i), t, p_i)           // 预测该视角的噪声
7.         ε_i' = v_i^(-1)(ε_i)                  // 逆变换回原空间
8.     end for
9.     
10.    // 融合多个噪声预测
11.    ε_combined = reduction(ε_1', ε_2', ..., ε_n')  // 默认使用 mean
12.    
13.    // 执行去噪步骤
14.    x_{t-1} = denoise_step(x_t, ε_combined)
15. end for
16. return x_0
```

#### 2.4.3 噪声预测融合机制

**平均融合（Mean Reduction）**：

$$\epsilon_{\text{combined}} = \frac{1}{n}\sum_{i=1}^{n} v_i^{-1}(\epsilon_\theta(v_i(x_t), t, p_i))$$

**数学原理**：
- 每个视角的噪声预测 $\epsilon_i$ 在变换后的空间中有效
- 通过逆变换 $v_i^{-1}$ 将噪声预测映射回原始空间
- 平均融合确保所有视角的约束都被满足

**为什么逆变换是必要的**：
- 噪声预测 $\epsilon_i$ 是在变换空间 $v_i(x_t)$ 中计算的
- 但去噪操作需要在原始空间 $x_t$ 中进行
- 因此需要将 $\epsilon_i$ 通过 $v_i^{-1}$ 映射回原空间

#### 2.4.4 视角变换的数学表示

**旋转变换**（以顺时针90°为例）：

$$R_{90°}(x)[i, j, :] = x[j, H-1-i, :]$$

其中 $H$ 是图像高度。

**水平翻转**：

$$F_h(x)[i, j, :] = x[i, W-1-j, :]$$

**颜色反转**：

$$\text{Negate}(x) = 1 - x$$

**逆变换**：
- 旋转的逆：$R_{90°}^{-1} = R_{-90°}$（逆时针90°）
- 翻转的逆：$F_h^{-1} = F_h$（自身）
- 反转的逆：$\text{Negate}^{-1} = \text{Negate}$（自身）

### 2.5 采样算法详解

#### 2.5.1 DDPM 随机采样

**DDPM（Denoising Diffusion Probabilistic Models）采样过程**：

对于 $t = T, T-1, ..., 1$：

$$x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t, c)\right) + \sigma_t z$$

其中：
- $\sigma_t = \beta_t$（方差调度）
- $z \sim \mathcal{N}(0, \mathbf{I})$（随机噪声）

**特点**：
- 随机性：每次采样结果不同
- 质量高：适合最终生成
- 速度慢：需要完整 $T$ 步

#### 2.5.2 DDIM 确定性采样

**DDIM（Denoising Diffusion Implicit Models）采样过程**：

$$x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\left(\frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t, t, c)}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t, t, c)$$

**特点**：
- 确定性：给定初始噪声，结果确定
- 可加速：可用更少步数（如 20 步）达到相似质量
- 适合超分辨率：保持低分辨率图像的结构

#### 2.5.3 噪声调度（Noise Schedule）

**线性调度**：
$$\beta_t = \frac{t}{T} \cdot \beta_{\max}$$

**余弦调度**（本项目使用）：
$$\bar{\alpha}_t = \frac{\cos^2(\pi t / 2T)}{\cos^2(\pi \cdot 0 / 2T)}$$

**优势**：
- 在去噪早期添加更多噪声
- 在去噪后期添加更少噪声
- 提高生成质量

### 2.6 级联超分辨率原理

#### 2.6.1 Stage I: 基础生成（64×64）

**输入**：文本嵌入 $c = \text{T5}(prompt)$

**过程**：
$$x_0^{64} = \text{DDPM-Sample}(\epsilon_\theta^{I}, c, T=30, \text{guidance\_scale}=10.0)$$

**多视角去噪**：
在每个去噪步骤 $t$：
1. 对 $x_t$ 应用视角变换：$v_1(x_t)$, $v_2(x_t)$
2. 分别预测噪声：$\epsilon_1 = \epsilon_\theta(v_1(x_t), t, p_1)$, $\epsilon_2 = \epsilon_\theta(v_2(x_t), t, p_2)$
3. 逆变换并融合：$\epsilon = \text{mean}(v_1^{-1}(\epsilon_1), v_2^{-1}(\epsilon_2))$
4. 执行去噪：$x_{t-1} = \text{denoise}(x_t, \epsilon)$

**特点**：
- 生成基础语义结构
- 快速（~6秒，30步）
- 低分辨率但包含主要特征
- 满足多视角约束

#### 2.6.2 Stage II: 超分辨率（64×64 → 256×256）

**输入**：低分辨率图像 $x_0^{64}$ 和文本嵌入 $c$

**过程**：
1. **上采样低分辨率图像**：$x_0^{64} \to x_0^{256}$（双线性插值）
2. **添加噪声**：$x_t^{256} = \sqrt{\bar{\alpha}_t}x_0^{256} + \sqrt{1-\bar{\alpha}_t}\epsilon$，其中 $t$ 对应 `noise_level=50`
3. **多视角去噪采样**：
$$x_0^{256} = \text{DDIM-Sample}(\epsilon_\theta^{II}, x_t^{256}, c, T=30, \text{noise\_level}=50)$$

**关键参数**：
- `noise_level=50`：对应时间步 $t=50$，控制添加的噪声量
  - 值越大：更多创造性，但可能偏离低分辨率图像
  - 值越小：更忠实于低分辨率图像，但创造性降低
  - 50 是平衡点

**多视角约束保持**：
在超分辨率过程中同样应用多视角去噪，确保：
- 变换后的高分辨率图像仍然满足多视角约束
- 高分辨率细节与低分辨率结构一致

#### 2.6.3 Stage III: 最终超分辨率（256×256 → 1024×1024）

**输入**：中等分辨率图像 $x_0^{256}$

**过程**：
$$x_0^{1024} = \text{Upscaler}(x_0^{256}, \text{noise\_level}=0)$$

**特点**：
- `noise_level=0`：不添加噪声，纯上采样
- 使用 Stable Diffusion x4 Upscaler
- 保持 Stage II 生成的语义内容
- 仅增强细节和清晰度，不改变整体结构
- 不应用多视角约束（因为 Stage III 是纯上采样）

### 2.7 文本引导机制

#### 2.7.1 T5 文本编码器

**T5-XXL 编码器架构**：
- 参数量：11B
- 输入：文本提示（tokenized）
- 输出：文本嵌入 $c \in \mathbb{R}^{77 \times 4096}$
  - 77：最大 token 长度
  - 4096：每个 token 的嵌入维度

**编码过程**：
$$c = \text{T5-Encoder}(\text{tokenize}(prompt))$$

**特点**：
- 强大的语言理解能力
- 支持复杂语义描述
- 长上下文理解（最多 77 tokens）

#### 2.7.2 交叉注意力机制

UNet 中的交叉注意力层将文本嵌入注入图像特征：

**多头交叉注意力**：

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

其中每个头：

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**参数来源**：
- $Q$：来自图像特征 $f_{\text{img}} \in \mathbb{R}^{H \times W \times d}$
- $K, V$：来自文本嵌入 $c \in \mathbb{R}^{77 \times 4096}$

**作用**：
- 将文本语义注入图像特征
- 在每个去噪步骤中，根据文本提示引导图像生成方向

#### 2.7.3 无分类器引导（Classifier-Free Guidance）

**原理**：
训练时同时学习条件模型 $\epsilon_\theta(x_t, t, c)$ 和无条件模型 $\epsilon_\theta(x_t, t, \emptyset)$。

**推理时引导公式**：

$$\tilde{\epsilon}_\theta(x_t, t, c) = \epsilon_\theta(x_t, t, \emptyset) + s \cdot (\epsilon_\theta(x_t, t, c) - \epsilon_\theta(x_t, t, \emptyset))$$

其中：
- $\epsilon_\theta(x_t, t, \emptyset)$：无条件噪声预测
- $\epsilon_\theta(x_t, t, c)$：条件噪声预测
- $s$：引导强度（guidance_scale）

**引导强度影响**（guidance_scale = 10.0）：

| 引导强度 $s$ | 效果 | 适用场景 |
|------------|------|---------|
| $s=1$ | 无引导，随机生成 | 探索性生成 |
| $s=5$ | 弱引导 | 需要一定创造性 |
| $s=10$ | 强引导（本项目） | 精确控制，高质量 |
| $s>15$ | 过度引导 | 可能产生伪影 |

**数学解释**：
引导公式可以重写为：

$$\tilde{\epsilon}_\theta = (1-s)\epsilon_\theta(x_t, t, \emptyset) + s \cdot \epsilon_\theta(x_t, t, c)$$

这相当于在无条件预测和条件预测之间进行插值，$s$ 越大，越偏向条件预测。

### 2.8 支持的视角变换详解

| 变换类型 | 数学表示 | 逆变换 | 视觉效果 | 适用场景 | 实现复杂度 |
|----------|----------|--------|----------|----------|-----------|
| identity | $I(x) = x$ | $I$ | 原始视角 | 基准视角 | 低 |
| rotate_180 | $R_{180°}(x)[i,j] = x[H-1-i, W-1-j]$ | $R_{180°}$ | 旋转180° | 对称错觉 | 低 |
| rotate_cw | $R_{90°}(x)[i,j] = x[j, H-1-i]$ | $R_{-90°}$ | 顺时针90° | 方向错觉（默认） | 低 |
| rotate_ccw | $R_{-90°}(x)[i,j] = x[W-1-j, i]$ | $R_{90°}$ | 逆时针90° | 方向错觉 | 低 |
| flip | $F_h(x)[i,j] = x[i, W-1-j]$ | $F_h$ | 水平翻转 | 镜像错觉 | 低 |
| negate | $N(x) = 1 - x$ | $N$ | 颜色反转 | 负片错觉 | 低 |
| skew | $S_k(x)$ | $S_k^{-1}$ | 倾斜变换 | 几何变形 | 中 |
| patch_permute | $P_p(x)$ | $P_p^{-1}$ | 块置换 | 拼图效果 | 高 |
| pixel_permute | $P_{pix}(x)$ | $P_{pix}^{-1}$ | 像素置换 | 抽象效果 | 高 |
| inner_circle | $C_{in}(x)$ | $C_{in}^{-1}$ | 内圆变换 | 圆形错觉 | 中 |
| square_hinge | $H_s(x)$ | $H_s^{-1}$ | 方形铰链 | 折叠效果 | 中 |
| jigsaw | $J(x)$ | $J^{-1}$ | 拼图重排 | 拼图错觉 | 高 |


## 三、系统设计

### 3.1 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                        用户接口层                            │
│                      (main.py - CLI)                        │
├─────────────────────────────────────────────────────────────┤
│                        业务逻辑层                            │
│               (generate.py - VisualAnagramGenerator)        │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │ 文本编码模块  │  │ 图像生成模块  │  │ 动画生成模块  │      │
│  │ T5 Encoder   │  │ Stage I/II/III│ │ Animator     │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
├─────────────────────────────────────────────────────────────┤
│                        配置管理层                            │
│                   (config.py / utils.py)                    │
├─────────────────────────────────────────────────────────────┤
│                        模型服务层                            │
│    ┌────────────────────────────────────────────────┐       │
│    │              HuggingFace Diffusers              │       │
│    │  DeepFloyd IF-I-L │ IF-II-L │ SD-x4-Upscaler   │       │
│    └────────────────────────────────────────────────┘       │
├─────────────────────────────────────────────────────────────┤
│                        基础设施层                            │
│           PyTorch │ CUDA │ Transformers │ Accelerate        │
└─────────────────────────────────────────────────────────────┘
```

## 四、实验与结果

### 4.1 实验环境

| 配置项 | 规格 | 说明 |
|--------|------|------|
| 操作系统 | Linux/Windows | 支持 CUDA 的操作系统 |
| CUDA | 11.8+ | GPU 计算支持 |
| Python | 3.10+ | 编程语言版本 |
| PyTorch | 2.0+ | 深度学习框架 |
| GPU | NVIDIA GPU | 支持 CUDA 的显卡 |
| 显存需求 | 最低 12GB | 优化后实际需求 |
| 系统内存 | 32GB+ | 推荐配置（用于 CPU Offload） |
| 磁盘空间 | 100GB+ | 模型文件缓存（约 40GB） |


### 4.3 生成效果示例

#### 示例 1: 旋转错觉

**配置参数**：
- 提示词 1: "painting of a Snow Mountain"
- 提示词 2: "painting of a fish"
- 变换类型: rotate_cw (顺时针旋转90°)
- 推理步数: 30
- 引导强度: 10.0
- 噪声等级: 50

![demo](./sd.png)

**生成结果**：
- **原始视角（identity）**：显示一幅雪山风景画，包含雪峰、山谷等元素
- **旋转视角（rotate_cw）**：将图像顺时针旋转90°后，呈现出一条鱼的形态，鱼的轮廓、鳍、眼睛等特征清晰可见
- **分辨率输出**：64×64（基础生成）→ 256×256（超分辨率）→ 1024×1024（最终输出）

**视觉效果分析**：
- 正向观看时，图像呈现完整的雪山场景，具有层次感和空间感
- 旋转后，相同的像素排列重新组织，形成鱼的形状，体现了多视角视觉错觉的核心特征
- 两个视角的语义内容完全不同，但共享相同的像素空间

---

## 五、项目结构

```
visual-anagrams-project/
│
├── main.py              # 程序入口，CLI 接口
├── generate.py          # 核心生成器类
├── config.py            # 配置参数
├── utils.py             # 工具函数
├── requirements.txt     # Python 依赖
├── README.md            # 项目说明
└── 项目报告.md           # 本报告
```


## 六、总结与展望

### 6.1 项目成果

1. 复现了基于 DeepFloyd IF 的多视角视觉错觉生成系统
2. 支持 12 种不同的几何变换类型
3. 通过内存优化策略，使系统可在消费级 GPU上运行
4. 提供完整的命令行工具和动画生成功能

---

## 参考文献

1. Geng, D., et al. "Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models." CVPR 2024.
2. Ho, J., et al. "Denoising Diffusion Probabilistic Models." NeurIPS 2020.
3. Saharia, C., et al. "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding." NeurIPS 2022.
4. DeepFloyd Lab. "IF: A Novel State-of-the-Art Open-Source Text-to-Image Model." 2023.

---

**报告完成日期**: 2025年12月


